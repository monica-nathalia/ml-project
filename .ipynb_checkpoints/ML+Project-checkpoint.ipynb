{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " (5 pts) Write a function that estimates the emission parameters from the training set using MLE (maximum likelihood estimation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(10 pts) One problem with estimating the emission parameters is that some words that appear in the\n",
    "test set do not appear in the training set. One simple idea to handle this issue is as follows. First,\n",
    "replace those words that appear less than k times in the training set with a special token #UNK#\n",
    "before training. This leads to a “modified training set”. We then use such a modified training set to\n",
    "train our model.\n",
    "During the testing phase, if the word does not appear in the “modified training set”, we replace that\n",
    "word with #UNK# as well.\n",
    "Set k to 3, implement this fix into your function for computing the emission parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(10 pts) Implement a simple sentiment analysis system that produces the tag\n",
    "y\n",
    "∗ = arg max\n",
    "y\n",
    "e(x|y)\n",
    "for each word x in the sequence.\n",
    "For all the four datasets EN, FR, CN, and SG, learn these parameters with train, and evaluate your\n",
    "system on the development set dev.in for each of the dataset. Write your output to dev.p2.out\n",
    "for the four datasets respectively. Compare your outputs and the gold-standard outputs in dev.out\n",
    "and report the precision, recall and F scores of such a baseline system for each dataset.\n",
    "The precision score is defined as follows:\n",
    "Precision =\n",
    "Total number of correctly predicted entities\n",
    "Total number of predicted entities\n",
    "The recall score is defined as follows:\n",
    "Recall =\n",
    "Total number of correctly predicted entities\n",
    "Total number of gold entities\n",
    "where a gold entity is a true entity that is annotated in the reference output file, and a predicted entity\n",
    "is regarded as correct if and only if it matches exactly the gold entity (i.e., both their boundaries and\n",
    "sentiment are exactly the same).\n",
    "4\n",
    "Finally the F score is defined as follows:\n",
    "F =\n",
    "2\n",
    "1/Precision + 1/Recall\n",
    "Note: in some cases, you might have an output sequence that consists of a transition from O to\n",
    "I-negative (rather than B-negative). For example, “O I-negative I-negative O”.\n",
    "In this case, the second and third words should be regarded as one entity with negative sentiment.\n",
    "You can use the evaluation script shared with you to calculate such scores. However it is strongly\n",
    "encouraged that you understand how the scores are calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "input = open()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
